{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05beda4f-5eae-4113-85e2-c6c85acab8a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "HDFS - Hadoop Distributed File System\n",
    "-----------------------------------------\n",
    "\n",
    "OS - File System\n",
    "==================\n",
    " | -> data structure - holding information about file //meta data\n",
    "\n",
    "notepad ->Hello ->file ->p1.txt ->Save # OK\n",
    "          ===== //plain text - data\n",
    " |\n",
    "OS - Filesystem | meta data - indexNumber - sector_range ... |\n",
    " |\n",
    "HardDisk (Storage) - Raw data\n",
    "-----------------------------------------------------------\n",
    "HDFS \n",
    "-----\n",
    "meta data - filename,index,block info ... //Not stores actual data\n",
    "\n",
    "emp.csv\n",
    "-----------\n",
    " |->Block1 ... data node \n",
    " |->Block2 ... data node\n",
    "\n",
    "\n",
    "1. master-slave\n",
    "2. peer to peer \n",
    "\n",
    "1. master-slave\n",
    "\n",
    "     [ master-node ] ---- Single Point of Communication (SPOC)\n",
    "             |\n",
    "     -------------------------|\n",
    "     |             |          |\n",
    " [slavenode1] [slavenode2] [slavenode3]\n",
    "\n",
    "2. peer to peer \n",
    "\n",
    "     [node1] <---> [node2] \n",
    "         |___[node3]___|\n",
    "    all the nodes are connected\n",
    "\n",
    "\n",
    "Enduser --> [edgeNode] --->[masterNode]\n",
    "                             |  ................ //meta-data - file\n",
    "                            ClientAPI-Calls()\n",
    "                             |\n",
    "                            slave1 slave2 ... slaveN \n",
    "                            .........//block data\n",
    "replication - copy\n",
    "\n",
    "[slavenode1]  [slavenode2]  [slavenode3]\n",
    "    B0 B1          B0 B2          B1 B2\n",
    "\n",
    "name node - master node - metadata file\n",
    "data node - slave node - actual files\n",
    "    \n",
    "ActiveNameNode(ANN)          Passive NameNode(PNN)\n",
    "\n",
    "\n",
    "--------------------------------------    // meta data - Filesystem\n",
    "   Index         |  PageNumber\n",
    "-----------------|--------------------\n",
    "1. About spark   |   1\n",
    "2. Pyspark       |  15\n",
    "3. Memory        |  26\n",
    "4. Process       |  89\n",
    "5. Device        |  123\n",
    "--------------------------------------\n",
    "\n",
    "1 ------------ 14  15------------------25\n",
    "     |                     |\n",
    "    Actual Data         info about\n",
    "    about spark           Pyspark\n",
    "     //raw data           raw data\n",
    "-------------------------------------------------------------------------------\n",
    "Apache spark ---->spark\n",
    "Open source analytics engine - large scale data processing\n",
    "\n",
    "L = [10,20,30,40,50] //Calculate sum the list -- core1 \n",
    "      Vs\n",
    "L = [ Billions of data ] //Calculate sum the list -- ? //use spark\n",
    "\n",
    "distributed cluster (multinode)\n",
    "In-memory \n",
    "realtime stream\n",
    "\n",
    "Data Process\n",
    "------------\n",
    "1. Batch process - stored \n",
    "2. Stream process - live stream data (ex: Logs from servers; sensor data from IOT;ATM)\n",
    "\n",
    "Appln ---> p1.log p2.log ..pn.log/\n",
    "|\n",
    "AppDir/p1.log p2.log ..pn.log\n",
    "\n",
    "spark.readStream.text(\"AppDir\") \n",
    "..\n",
    "python(API) + spark = Pyspark\n",
    "scala + spark\n",
    "java + spark\n",
    "\n",
    "python ---- API ---[Spark]  = PySpark\n",
    "\n",
    " |[Sql] [stream] [mlib] [graph]\n",
    " |-----------------------------|\n",
    " | spark - core\n",
    "\n",
    "lowlevel API - RDD\n",
    "structured API - DF,SQL\n",
    "lib & eco systems\n",
    "\n",
    "\n",
    "+-----------------+\n",
    "|  Sprak Context  | <-----> [  ] <---> [slaveNode]\n",
    "+-----------------+\n",
    "\n",
    "\n",
    "+------------+\n",
    "| [][] jvm\n",
    "|---------\n",
    "| [][] \n",
    "|---------\n",
    "| [][]\n",
    "\n",
    "partition - data chunks\n",
    "transformation\n",
    "\n",
    "\n",
    "Two main abstractions of apache spark\n",
    "-------------------------------------\n",
    "1. Resilient Distributed Datasets (RDD) - immutable \n",
    "2. Directed Ascylic Graph (DAG)\n",
    "|\n",
    "jdk - jdk 1.8 jvm\n",
    "pyspark\n",
    "-------------------------------------------------\n",
    "step 1: import pyspark module and load session class\n",
    "step 2: create a spark session\n",
    "step 3: read streaming data from a directory\n",
    "step 4: split \n",
    "..\n",
    "step 5: steam query\n",
    "step 6: \n",
    "----------------------------------------------------\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "SparkSession.builder.appName(\"data stream\").getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "964fb9e3-e1e0-4a93-b01f-b984d451581e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"PYSPARK_PYTHON\"] = \"C:\\\\Users\\karth\\AppData\\Local\\Programs\\Python\\Python310\\python.exe\"\n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON\"] = \"C:\\\\Users\\karth\\AppData\\Local\\Programs\\Python\\Python310\\python.exe\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "77af3e6e-291e-4420-ba1b-47e3862e9614",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://paka:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v4.0.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>App</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x2b18092c980>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "SparkSession.builder.appName(\"App\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b40dd3c0-1270-4779-97f8-70d0529ab3e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://paka:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v4.0.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>App</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x2b18092c980>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName(\"App\").getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a4f82b3d-e3db-433f-ad1e-589049ca1348",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[pid: bigint, pname: string, pcost: bigint]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName('example1').getOrCreate()\n",
    "product=[(101,'pA',1000),(102,'pB',2000),(103,'pC',3000)]\n",
    "columns=['pid','pname','pcost'] # schema\n",
    "\n",
    "df = spark.createDataFrame(data=product,schema=columns)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cf959538-a5ef-46c2-a203-045648ba19d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- pid: long (nullable = true)\n",
      " |-- pname: string (nullable = true)\n",
      " |-- pcost: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d063ce6d-6770-4a72-a7af-eea746b34130",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Download Java\n",
    "https://www.oracle.com/in/java/technologies/javase/javase8-archive-downloads.html#license-lightbox \n",
    "<or>\n",
    "https://adoptium.net/temurin/releases/?version=17\n",
    "\n",
    "# 2. Download python 3.10 \n",
    "https://www.python.org/ftp/python/3.10.9/python-3.10.9-amd64.exe\n",
    "\n",
    "# 3. Download Apache Spark\n",
    "https://spark.apache.org/downloads.html\n",
    "\n",
    "# 4. Download Hadoop Winutils and Hadoop.dll <= only windows\n",
    "https://dlcdn.apache.org/spark/spark-4.0.0/spark-4.0.0-bin-hadoop3.tgz\n",
    "\n",
    "# 5. set env variables \n",
    "\n",
    "###########################\n",
    "RHL8/CentOS8/OL8 \n",
    "------------------\n",
    "dnf install python3-pip java-11-openjdk\n",
    "pip3 install pyspark\n",
    "vi ~/.bashrc\n",
    "export JAVA_HOME=/usr/lib/jvm/java-1.8.0-openjdk-1.8.0.462.b08-2.0.1.el8.x86_64/jre\n",
    ":wq\n",
    "source ~/.bashrc \n",
    "echo $JAVA_HOME\n",
    "----------------------------\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
